<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="A detailed guide, best practices, and performance tuning parameters for VMware using iSCSI storage. Everything you need to know for optimal configuration.">
<meta name="keywords" content=" vmware">
<title>Blockbridge VMware Storage Guide | Blockbridge Knowledgebase</title>
<!-- Google Fonts -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans&display=swap">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto&display=swap">

<link rel="stylesheet" href="/css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="/css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="/css/customstyles.css">
<link rel="stylesheet" href="/css/sidebar.css">
<link rel="stylesheet" href="/css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="/css/theme-blockbridge.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="/js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/2.0.0/anchor.min.js"></script>
<script src="/js/toc.js"></script>
<script src="/js/customscripts.js"></script>

<link rel="shortcut icon" href="/images/bb-favicon.ico" type="image/png">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="blockbridge-kb" href="/feed.xml">

    <script>
        $(document).ready(function() {
          // Initialize navgoco with default options
          $("#mysidebar").navgoco({
            caretHtml: '',
            accordion: false,
            openClass: 'active', // open
            save: false, // leave false or nav highlighting doesn't work right
            cookie: {
              name: 'navgoco',
              expires: false,
              path: '/'
            },
            slide: {
              duration: 400,
              easing: 'swing'
            }
          });

          $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    

</head>
<body>
  <!-- Navigation -->
<nav class="navbar navbar-inverse navbar-fixed-top">
    <div class="container-fluid topnavlinks">
        <div class="navbar-header">
            <div class="navbar-brand"><svg class="bb-logo" width="100%" xmlns="http://www.w3.org/2000/svg" version="1.1" id="Layer_1" x="0px" y="0px" viewBox="0 0 346 40" enable-background="new 0 0 346 40" space="preserve">
  <path fill="#FFFFFF" d="M85.328613,31.876953L90.526855,0.72998h12.858887c2.340332,0,4.255859,0.209961,5.745117,0.62793 c1.489746,0.418457,2.739746,1.274414,3.750977,2.566895c1.010742,1.293457,1.515625,2.806641,1.515625,4.541016 c0,1.902344-0.600098,3.518066-1.800781,4.848633c-1.201172,1.331543-2.8125,2.225098-4.833496,2.681152 c3.419922,1.20166,5.129883,3.461426,5.129883,6.777832c0,2.416992-0.817383,4.540039-2.451172,6.365234 s-4.350586,2.738281-8.150391,2.738281H85.328613z M95.493652,24.986328h4.652832c1.444336,0,2.440918-0.304688,2.987793-0.913086 c0.547363-0.608398,0.821777-1.269531,0.821777-1.985352c0-0.881836-0.300781-1.59375-0.902344-2.132812 c-0.601074-0.540039-1.495117-0.810547-2.681641-0.810547h-3.90332L95.493652,24.986328z M97.557129,12.618652h3.563477 c2.847656,0,4.271484-0.943359,4.271484-2.82959c0-1.657715-1.127441-2.487305-3.382324-2.487305h-3.564941L97.557129,12.618652z"/>
  <path fill="#FFFFFF" d="M129.130859,0.72998l-5.196777,31.146973h-8.76123l5.19873-31.146973H129.130859z"/>
  <path fill="#FFFFFF" d="M140.753906,32.561523c-3.20752,0-5.901367-0.954102-8.08252-2.864258 c-2.181152-1.908203-3.271484-4.513672-3.271484-7.814453c0-3.970703,1.265137-7.23291,3.79541-9.788574 c2.530762-2.555664,5.726562-3.833496,9.587402-3.833496c3.526367,0,6.32666,1,8.401367,3 c2.075195,2.000977,3.112305,4.469238,3.112305,7.405273c0,4.365234-1.333984,7.772461-4.000977,10.22168 C147.627441,31.336914,144.447266,32.561523,140.753906,32.561523z M141.118652,26.948242 c1.595703,0,2.678711-1.186523,3.248535-3.55957c0.570312-2.373047,0.855469-4.357422,0.855469-5.956055 c0-2.433105-0.882324-3.650879-2.64502-3.650879c-1.56543,0-2.648438,1.213867-3.249023,3.640137 c-0.600586,2.426758-0.900879,4.408203-0.900879,5.943359C138.427734,25.754883,139.324707,26.948242,141.118652,26.948242z"/>
  <path fill="#FFFFFF" d="M171.578125,22.932617l6.953125,1.710938c-1.246094,2.981445-2.880859,5.046875-4.900879,6.195312 c-2.021973,1.147461-4.111816,1.722656-6.27002,1.722656c-3.176758,0-5.771973-1.008789-7.786133-3.026367 c-2.013672-2.017578-3.020508-4.610352-3.020508-7.776367c0-3.608398,1.188965-6.79834,3.567871-9.570312 c2.378906-2.77002,5.528809-4.156738,9.450195-4.156738c2.842285,0,5.251465,0.855957,7.227051,2.567871 c1.975586,1.710938,2.963867,3.912598,2.963867,6.605469l-7.819824,0.913086c0-2.494629-0.843262-3.742188-2.530273-3.742188 c-1.444336,0-2.470215,1.021484-3.078125,3.063477s-0.912109,3.825195-0.912109,5.349609 c0,2.255859,0.851074,3.383789,2.553711,3.383789C169.647949,26.172852,170.848633,25.092773,171.578125,22.932617z"/>
  <path fill="#FFFFFF" d="M208.717773,8.836426l-4.012695,3.602051c-2.220703,1.990234-3.77832,3.441895-4.675781,4.353516 l5.541992,15.084961h-8.745117l-3.182617-9.854492l-3.962891,3.580078l-1.051758,6.274414h-8.113281l5.198242-31.146973h8.134766 l-2.888672,17.239746l1.666016-1.666016c0.500977-0.533203,0.926758-0.958984,1.275391-1.279297l6.775391-6.187988H208.717773z"/>
  <path fill="#FFFFFF" d="M221.697266,0.72998l-1.841797,10.977539c1.200195-1.430664,2.269531-2.374023,3.210938-2.830078 c0.941406-0.456543,1.998047-0.685547,3.167969-0.685547c2.216797,0,3.993164,0.787109,5.330078,2.361816 c1.335938,1.574219,2.004883,3.898438,2.004883,6.970703c0,3.773438-1.056641,7.195312-3.168945,10.268555 c-2.112305,3.072266-5.043945,4.608398-8.796875,4.608398c-3.555664,0-6.032227-1.459961-7.429688-4.380859l-2.579102,3.856445 h-3.994141l5.199219-31.146973H221.697266z M217.824219,23.800781c0,0.851562,0.060547,1.4375,0.182617,1.756836 c0.12207,0.318359,0.387695,0.623047,0.796875,0.912109s0.917969,0.432617,1.524414,0.432617 c0.774414,0,1.438477-0.253906,1.992188-0.764648c0.554688-0.508789,1.085938-1.638672,1.59375-3.387695 c0.508789-1.75,0.763672-3.613281,0.763672-5.59082c0-2.296387-0.802734-3.445312-2.40918-3.445312 c-1.545898,0-2.65918,1.172852-3.34082,3.517578L217.824219,23.800781z"/>
  <path fill="#FFFFFF" d="M238.8125,8.831055h7.674805l-0.919922,5.55957c1.598633-4.163574,4.042969-6.244141,7.333008-6.244141 c0.258789,0,0.616211,0.037598,1.073242,0.114258l-1.025391,8.214844c-0.791016-0.046875-1.31543-0.069336-1.574219-0.069336 c-1.900391,0-3.294922,0.445312-4.18457,1.333984s-1.523438,2.463867-1.901367,4.726562l-1.567383,9.410156h-8.761719 L238.8125,8.831055z"/>
  <path fill="#FFFFFF" d="M266.025391,8.831055l-3.857422,23.045898h-8.945312l3.857422-23.045898H266.025391z M267.503906,0 l-1.134766,6.777344h-8.946289L258.556641,0H267.503906z"/>
  <path fill="#FFFFFF" d="M294.807617,0.72998l-5.198242,31.146973h-8.181641l0.564453-3.485352 c-1.884766,2.780273-4.164062,4.169922-6.838867,4.169922c-2.082031,0-3.875-0.821289-5.379883-2.464844 c-1.503906-1.642578-2.256836-3.993164-2.256836-7.049805c0-3.789062,1.041992-7.210449,3.125977-10.269043 c2.083008-3.056641,4.751953-4.585938,8.006836-4.585938c2.783203,0,4.660156,1.096191,5.633789,3.287598l1.787109-10.749512 H294.807617z M283.329102,17.021484c0.044922-0.243164,0.067383-0.50293,0.067383-0.776855 c0-0.65332-0.232422-1.20752-0.695312-1.663574c-0.461914-0.456543-1.019531-0.68457-1.672852-0.68457 c-1.5625,0-2.688477,1.205566-3.378906,3.616211c-0.69043,2.412109-1.035156,4.416016-1.035156,6.012695 c0,2.099609,0.814453,3.149414,2.444336,3.149414c1.614258,0,2.6875-1.048828,3.220703-3.147461L283.329102,17.021484z"/>
  <path fill="#FFFFFF" d="M320.798828,2.281738l-0.547852,5.75c-0.911133-0.105469-1.595703-0.15918-2.051758-0.15918 c-1.686523,0-2.834961,0.615723-3.442383,1.848633c2.356445,1.414062,3.53418,3.247559,3.53418,5.498535 c0,1.962891-0.766602,3.705078-2.299805,5.224609c-1.533203,1.521484-4.348633,2.282227-8.446289,2.282227 c-0.866211,0-1.579102-0.030273-2.140625-0.09082c-0.819336-0.091797-1.396484-0.136719-1.729492-0.136719 c-0.554688,0-1.010742,0.162109-1.37207,0.487305s-0.541016,0.699219-0.541016,1.121094 c0,0.363281,0.121094,0.65625,0.364258,0.882812s0.524414,0.359375,0.84375,0.395508 c0.319336,0.039062,1.686523,0.094727,4.104492,0.169922c2.644531,0.092773,4.357422,0.1875,5.140625,0.287109 c0.783203,0.098633,1.645508,0.400391,2.587891,0.902344c0.941406,0.50293,1.683594,1.208008,2.222656,2.114258 s0.80957,1.922852,0.80957,3.050781c0,2.163086-0.958008,4.052734-2.873047,5.667969 C313.046875,39.192383,309.421875,40,304.086914,40c-8.071289,0-12.106445-1.856445-12.106445-5.567383 c0-1.931641,1.37207-3.415039,4.117188-4.449219c-1.483398-0.989258-2.224609-2.213867-2.224609-3.673828 c0-2.555664,1.864258-4.373047,5.594727-5.453125c-2.134766-1.202148-3.201172-2.913086-3.201172-5.134766 c0-2.022949,0.880859-3.772461,2.644531-5.248047c1.763672-1.474609,4.727539-2.212891,8.891602-2.212891 c1.277344,0,2.470703,0.11377,3.580078,0.341797c0.65332-4.304688,3.009766-6.457031,7.068359-6.457031 C319.058594,2.145508,319.841797,2.19043,320.798828,2.281738z M300.527344,31.876953 c-0.865234,0.516602-1.296875,1.049805-1.296875,1.59668c0,1.15625,1.896484,1.734375,5.688477,1.734375 c1.865234,0,3.21875-0.118164,4.061523-0.353516c0.84082-0.235352,1.261719-0.696289,1.261719-1.380859 c0-0.456055-0.197266-0.756836-0.59082-0.901367c-0.395508-0.143555-1.251953-0.24707-2.571289-0.306641 C303.256836,32.09668,301.073242,31.967773,300.527344,31.876953z M307.154297,18.574219 c0.96875,0,1.729492-0.342773,2.282227-1.027344s0.828125-1.459961,0.828125-2.327148 c0-1.688477-0.832031-2.533203-2.49707-2.533203c-1.029297,0-1.813477,0.351074-2.350586,1.050781 s-0.805664,1.52832-0.805664,2.486328c0,0.745117,0.242188,1.323242,0.726562,1.734375S306.427734,18.574219,307.154297,18.574219z"/>
  <path fill="#FFFFFF" d="M345.650391,22.042969h-15.868164c-0.076172,0.625977-0.114258,1.092773-0.114258,1.397461 c0,1.008789,0.323242,1.796875,0.96875,2.362305c0.645508,0.566406,1.462891,0.848633,2.451172,0.848633 c1.869141,0,3.191406-0.972656,3.966797-2.920898l7.751953,1.255859c-1.00293,2.256836-2.625977,4.084961-4.868164,5.480469 c-2.241211,1.396484-4.730469,2.094727-7.466797,2.094727c-3.43457,0-6.220703-0.976562-8.355469-2.931641 c-2.135742-1.955078-3.204102-4.621094-3.204102-7.999023c0-3.665039,1.262695-6.810547,3.786133-9.434082 c2.522461-2.624512,5.821289-3.936035,9.894531-3.936035c3.450195,0,6.197266,1.01709,8.242188,3.052246 c2.043945,2.035156,3.06543,4.737793,3.06543,8.109863C345.900391,20.182617,345.816406,21.055664,345.650391,22.042969z M337.350586,17.615234c0.076172-0.470703,0.115234-0.835938,0.115234-1.094727c0-0.821777-0.255859-1.536133-0.763672-2.144531 c-0.510742-0.608398-1.220703-0.912598-2.132812-0.912598c-0.972656,0-1.853516,0.368164-2.644531,1.106445 s-1.276367,1.753418-1.458984,3.04541H337.350586z"/>
  <path fill-rule="evenodd" clip-rule="evenodd" fill="#4F8CCA" d="M34.401367,0.722168h22.350098c0,0,1.194336,0,1.15625,1.239258 L53.055664,30.75c-0.20752,1.061523-1.129395,1.125977-1.129395,1.125977h-22.41748c0,0-1.026367,0-0.961914-1.219727 l4.873047-28.866211C33.411133,0.790039,34.401367,0.722168,34.401367,0.722168z"/>
  <path fill-rule="evenodd" clip-rule="evenodd" fill="#6AA9DC" d="M62.686035,0.722168h22.350098c0,0,1.194336,0,1.15625,1.239258 L81.34082,30.75c-0.20752,1.061523-1.129883,1.125977-1.129883,1.125977H57.793945c0,0-1.026855,0-0.961914-1.219727 l4.873047-28.866211C61.696289,0.790039,62.686035,0.722168,62.686035,0.722168z"/>
  <path fill-rule="evenodd" clip-rule="evenodd" fill="#3B6BA0" d="M5.856934,0.722168h22.350098c0,0,1.194336,0,1.15625,1.239258 L24.511719,30.75c-0.20752,1.061523-1.129883,1.125977-1.129883,1.125977H0.964844c0,0-1.026855,0-0.961914-1.219727 L4.875977,1.790039C4.867188,0.790039,5.856934,0.722168,5.856934,0.722168z"/>
</svg></div>
        </div>
    </div>
    <!-- /.container -->
</nav>


  <!-- Page Content -->
  <div class="container-fluid">
    <div id="main">
      <!-- Content Row -->
      <div class="row">

        <div class="col-body">
          <!-- row-oriented flex container -->
          

          <!-- Sidebar Column -->
          <nav class="col-nav hidden-sm hidden-xs" id="tg-sb-sidebar">
            

<ul id="sidebar" class="sidebar">
  
  
  
  
  <li>
      <a href="#blockbridge-vmware-storage-guide">ABOUT THIS GUIDE</a>
      <ul>
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#deployment--tuning-quickstart">DEPLOYMENT & TUNING QUICKSTART</a>
      <ul>
          
          
          
          <li><a href="#vmware-iscsi-networking">VMware iSCSI Networking</a></li>
          
          
          
          
          
          
          <li><a href="#blockbridge-virtual-storage">Blockbridge Virtual Storage</a></li>
          
          
          
          
          
          
          <li><a href="#vmware-storage-devices">VMware Storage Devices</a></li>
          
          
          
          
          
          
          <li><a href="#vmware-datastore--vmfs">VMware Datastore & VMFS</a></li>
          
          
          
          
          
          
          <li><a href="#vmware-system-settings">VMware System Settings</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#deployment-planning">DEPLOYMENT PLANNING</a>
      <ul>
          
          
          
          <li><a href="#datastores-complexes--luns">Datastores, Complexes & LUNS</a></li>
          
          
          
          
          
          
          <li><a href="#vmware-storage-limits">VMware Storage Limits</a></li>
          
          
          
          
          
          
          <li><a href="#storage-io-control">Storage I/O Control</a></li>
          
          
          
          
          
          
          <li><a href="#storage-distributed-resource-scheduling">Storage Distributed Resource Scheduling</a></li>
          
          
          
          
          
          
          <li><a href="#storage-performance-classes">Storage Performance Classes</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#networking--storage">NETWORKING & STORAGE</a>
      <ul>
          
          
          
          <li><a href="#vmware-multipath-networking">VMware Multipath Networking</a></li>
          
          
          
          
          
          
          <li><a href="#lacp--bonded-networking">LACP & Bonded Networking</a></li>
          
          
          
          
          
          
          <li><a href="#provisioning-iscsi-storage">Provisioning iSCSI Storage</a></li>
          
          
          
          
          
          
          <li><a href="#vmware-initiator-configuration">VMware Initiator Configuration</a></li>
          
          
          
          
          
          
          <li><a href="#creating-a-vmfs-datastore">Creating a VMFS Datastore</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#host-tuning">HOST TUNING</a>
      <ul>
          
          
          
          <li><a href="#iscsi-lun-queue-depth">iSCSI LUN Queue Depth</a></li>
          
          
          
          
          
          
          <li><a href="#schednumreqoutstanding-depth">SchedNumReqOutstanding Depth</a></li>
          
          
          
          
          
          
          <li><a href="#queue-depth-full">Queue Depth Full</a></li>
          
          
          
          
          
          
          <li><a href="#round-robin-path-selection-iops-limit">Round Robin Path Selection IOPS Limit</a></li>
          
          
          
          
          
          
          <li><a href="#vaai-commands">VAAI Commands</a></li>
          
          
          
          
          
          
          <li><a href="#ats-heartbeating-vmfs-6">ATS Heartbeating (VMFS-6)</a></li>
          
          
          
          
          
          
          <li><a href="#halt-vms-on-out-of-space-conditions">Halt VMs on Out-of-Space Conditions</a></li>
          
          
          
          
          
          
          <li><a href="#jumbo-frames">Jumbo Frames</a></li>
          
          
          
          
          
          
          <li><a href="#iscsi-login-timeout">iSCSI Login Timeout</a></li>
          
          
          
          
          
          
          <li><a href="#tcp-delayedack">TCP DelayedAck</a></li>
          
          
          
          
          
          
          <li><a href="#large-receive-offload-maximum-length">Large Receive Offload Maximum Length</a></li>
          
          
          
          
          
          
          <li><a href="#nic-interrupt-balancing">NIC Interrupt Balancing</a></li>
          
          
          
          
          
          
          <li><a href="#mellanox-specific-optimizations">Mellanox Specific Optimizations</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a href="#guest-tuning">GUEST TUNING</a>
      <ul>
          
          
          
          <li><a href="#paravirtual-scsi-adapter">Paravirtual SCSI Adapter</a></li>
          
          
          
          
          
          
          <li><a href="#virtual-machine-encryption">Virtual Machine Encryption</a></li>
          
          
          
          
          
          
          <li><a href="#zeroing-policies">Zeroing Policies</a></li>
          
          
          
          
          
          
          <li><a href="#guest-io-latency--consistency">Guest I/O Latency & Consistency</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>

          </nav>
          

          <!-- Content Column -->
          <main class="col-content">
            <div class="post-header">
   <h1 class="post-title-main">Blockbridge VMware Storage Guide</h1>
</div>



<div class="post-content">

   

    


    

   <p>This guide provides technical details for deploying VMware ESXi and VMware
vSphere clusters on Blockbridge iSCSI storage.</p>

<p>Most readers will want to start with the <strong><a href="#deployment--tuning-quickstart">Deployment and Tuning
Quickstart</a></strong> section. It’s an ordered list of
configuration and tuning steps, and it’s the fastest path to an optimal
installation. The rest of the document provides detail on all aspects of using
VMware with Blockbridge:</p>

<ul>
  <li>
    <p><strong><a href="#deployment-planning">Deployment Planning</a></strong> is a discussion of how to
  plan your VMFS datastores for performance and flexibility. It describes how
  best to configure VMware’s Storage I/O Control and Storage Distributed
  Resource Scheduler features for use with all-flash storage.</p>
  </li>
  <li>
    <p><strong><a href="#networking--storage">Networking &amp; Storage</a></strong> provides
  administration, provisioning, and configuration details. The bulk of the
  content is devoted to networking requirements for multipath storage
  access. Additional topics include how to provision storage with Blockbridge
  and how to connect it to your ESXi hosts.</p>
  </li>
  <li>
    <p><strong><a href="#host-tuning">Host Tuning</a></strong> enumerates recommendations for host-level
  ESXi parameters that affect the performance, features, and resiliency of
  iSCSI storage.</p>
  </li>
  <li>
    <p><strong><a href="#guest-tuning">Guest Tuning</a></strong> offers additional recommendations for
  achieving high performance from guest VMs.</p>
  </li>
</ul>

<hr />

<h1 id="deployment--tuning-quickstart">DEPLOYMENT &amp; TUNING QUICKSTART</h1>

<h2 id="vmware-iscsi-networking">VMware iSCSI Networking</h2>

<ol>
  <li>
    <p><strong>Configure VMware networking and the VMware iSCSI adapter to
support multiple storage paths.</strong> <a href="#vmware-multipath-networking"><strong>ⓘ</strong></a></p>

    <ul>
      <li>If your network interface ports are on the same subnet, create iSCSI Port
Bindings.</li>
      <li>If your network interface ports are on different subnets, no additional
virtual networking is required.</li>
    </ul>
  </li>
  <li>
    <p><strong>Configure Jumbo Frames</strong> as needed. <a href="#jumbo-frames"><strong>ⓘ</strong></a></p>
  </li>
  <li>
    <p><strong>Increase the iSCSI LUN Queue Depth.</strong> <a href="#iscsi-lun-queue-depth"><strong>ⓘ</strong></a></p>

    <ul>
      <li>This setting has been found to increase performance.</li>
      <li>Requires a reboot to take effect.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli system module parameters set -m iscsi_vmk -p iscsivmk_LunQDepth=192
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Increase the iSCSI Login Timeout to ride out LUN failovers.</strong> <a href="#iscsi-login-timeout"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli iscsi adapter param set --adapter=vmhba64 --key=LoginTimeout --value=60
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Increase the Large Receive Offload Maximum Length.</strong> <a href="#large-receive-offload-maximum-length"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcfg-advcfg -s 65535 /Net/VmxnetLROMaxLength
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Verify that NIC Interrupt Balancing is Enabled.</strong> <a href="#nic-interrupt-balancing"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli system settings kernel list | grep intrBalancingEnabled
 intrBalancingEnabled                     Bool    true           ...
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Apply Mellanox ConnectX-3 NIC Tuning.</strong> <a href="#mellanox-specific-optimizations"><strong>ⓘ</strong></a></p>

    <ul>
      <li>Repeat for each interface port.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli network nic ring current set -r 4096 -n vmnicX
 esxcli network nic coalesce set -a false -n vmnicX
 esxcli network nic coalesce set –-tx-usecs=0 –-rx-usecs=3 –-adaptive-rx=false -n vmnicX
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Apply Mellanox ConnectX-4,5+ NIC Tuning.</strong> <a href="#mellanox-specific-optimizations"><strong>ⓘ</strong></a></p>

    <ul>
      <li>Repeat for each interface port.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli network nic ring current set -r 4096 -n vmnicX
 esxcli network nic coalesce set –-tx-usecs=0 –-adaptive-rx=true -n vmnicX
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="blockbridge-virtual-storage">Blockbridge Virtual Storage</h2>

<ol>
  <li>
    <p><strong>Create a global iSCSI initiator profile.</strong></p>

    <ul>
      <li>Register the iSCSI qualified name (IQN) of each VMware iSCSI adapter.</li>
      <li>Configure CHAP authentication credentials.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> bb profile create --label 'cluster profile' --initiator-login 'esx' --initiator-pass ************
 bb profile initiator add --profile 'cluster profile' --iqn iqn.1998-01.com.vmware:bb-cluster-4-0b2f0b43
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Provision a virtual storage service.</strong></p>

    <ul>
      <li>We recommend a single VMware datastore per Blockbridge complex for
optimal performance.</li>
      <li>Create the storage service from the “system” account on the Blockbridge
GUI to guarantee datastore placement.</li>
    </ul>
  </li>
  <li>
    <p><strong>Create a virtual disk within the virtual service.</strong></p>

    <ul>
      <li>We recommend a single Blockbridge virtual disk per VMware datastore.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> bb disk create --vss cx1:nvme --label ds1 --capacity 1TiB
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Create an iSCSI target within the virtual service.</strong></p>

    <ul>
      <li>Add a LUN mapping for your virtual disk.</li>
      <li>Insert your global iSCSI initiator profile into the access control list.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> bb target create --vss cx1:nvme --label target1
 bb target acl add --target target1 --profile cluster-profile
 bb target lun map --target target1 --disk ds1
</code></pre></div>    </div>
  </li>
</ol>

<div class="alert alert-success" role="alert"><i class="fa fa-check-square-o"></i> <b>Tip:</b> See <a href="#provisioning-iscsi-storage">Provisioning iSCSI Storage</a> for
detailed information on Blockbridge virtual storage configuration.</div>

<h2 id="vmware-storage-devices">VMware Storage Devices</h2>

<ol>
  <li>
    <p><strong>Add a Dynamic Discovery Target.</strong></p>

    <ul>
      <li>Choose one of the Blockbridge target portals - VMware will find the others.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli iscsi adapter discovery sendtarget add --adapter=vmhba64 --address=172.16.200.44:3260
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Increase the SchedNumReqOutstanding Depth for each storage device.</strong> <a href="#schednumreqoutstanding-depth"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli storage core device set --sched-num-req-outstanding=192 \
   --device=naa.60a010a0b139fa8b1962194c406263ad
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Set the Round-Robin path selection policy for each storage device.</strong> <a href="#vmware-initiator-configuration"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp device set --psp=VMW_PSP_RR --device=naa.60a010a071105fae1962194c40626ca8
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Lower the Round Robin Path Selection IOPS Limit for each storage
device.</strong> <a href="#round-robin-path-selection-iops-limit"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp psp roundrobin deviceconfig set --type=iops --iops=8 \
  --device=naa.60a010a03ff1bb511962194c40626cd1
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Confirm that Queue Full Sample Size and Threshold are 0.</strong> <a href="#queue-depth-full"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli storage core device list | grep 'Queue Full'
    Queue Full Sample Size: 0
    Queue Full Threshold: 0
    Queue Full Sample Size: 0
    Queue Full Threshold: 0
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Use this bash script helper to apply esxcli commands to all devices.</strong></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> for dev in $(esxcli storage nmp device list | egrep ^naa); do
   esxcli ... --device=${dev}
 done
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="vmware-datastore--vmfs">VMware Datastore &amp; VMFS</h2>

<ol>
  <li>
    <p><strong>Create a VMware datastore for each Blockbridge
disk.</strong> <a href="#creating-a-vmfs-datastore"><strong>ⓘ</strong></a></p>

    <ul>
      <li>Use VMFS version 6 or higher.</li>
    </ul>
  </li>
</ol>

<h2 id="vmware-system-settings">VMware System Settings</h2>

<p>Optionally, validate that the following system-wide settings retain their
default values.</p>

<ol>
  <li>
    <p><strong>Confirm that VAAI Commands are Enabled.</strong> <a href="#vaai-commands"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli  system settings advanced list -o /VMFS3/HardwareAcceleratedLocking
 ...
    Int Value: 1
 esxcli  system settings advanced list -o /DataMover/HardwareAcceleratedMove
 ...
    Int Value: 1
 esxcli  system settings advanced list -o /DataMover/HardwareAcceleratedInit
 ...
    Int Value: 1
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Confirm that the SCSI “Atomic Test and Set” Command is Used.</strong> <a href="#optimized-ats-heartbeating-vmfs-6"><strong>ⓘ</strong></a></p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli  system settings advanced list -o /VMFS3/UseATSForHBOnVMFS5
 ...
    Int Value: 1
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Confirm that VMs are Halted on Out of Space Conditions.</strong> <a href="#halt-vms-on-out-of-space-conditions"><strong>ⓘ</strong></a></p>

    <ul>
      <li>This option should be disabled.</li>
    </ul>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> esxcli  system settings advanced list -o /Disk/ReturnCCForNoSpace
 ...
    Int Value: 0
</code></pre></div>    </div>
  </li>
</ol>

<hr />

<h1 id="deployment-planning">DEPLOYMENT PLANNING</h1>

<h2 id="datastores-complexes--luns">Datastores, Complexes &amp; LUNS</h2>

<p>Historically, VMware’s storage performance is entirely limited by the backend
array. In days of HDD and hybrid HDD/SSD storage systems, many administrators
used a large number of low capacity LUNs to improve performance. The
performance improvement was attributed to a better distribution of I/O across
high-latency media.</p>

<p>With All-SSD and All-NVMe arrays, VMware’s iSCSI initiator is likely the
primary performance bottleneck, especially for high-IOPS workloads. When
operating an ESXi host in the range of hundreds of thousands of IOPS, you’ll
see a significant rise in CPU utilization and NIC interrupt processing
overhead. One of the best ways to ensure consistent storage performance is to
have adequate CPU resources available for the storage subsystem. You may also
unlock additional initiator-side concurrency benefits by doubling up on the
number of VMFS datastores you build from a Blockbridge dataplane complex. While
it’s not a guaranteed win, it may be worth an experiment if you are equipped
with high-performance servers.</p>

<div class="alert alert-success" role="alert"><i class="fa fa-check-square-o"></i> <b>Tip:</b> Our base recommendation is to <strong>provision one
Blockbridge LUN for each Blockbridge dataplane complex and create a single
VMFS-6 datastore from it</strong>. If you’re deploying with a multi-complex
Blockbridge dataplane, create one LUN and VMFS-6 datastore for each dataplane
complex: each complex provides an independent performance and failure domain.</div>

<p>We <strong>do not</strong> recommend incorporating multiple LUNs into a single
datastore. VMFS extents are not stripes. You are not likely to realize any
additional performance with multiple extents. Plus, Storage I/O Control will
not work on datastores with multiple extents.</p>

<p>For Storage DRS, using a greater number of smaller LUNs can give VMware’s
algorithm more leeway to achieve a balanced performance and capacity
solution. However, making the datastores too small could force additional
vMotions and create dead spaces that aren’t easily recaptured. It’s best to
avoid slicing and dicing the storage too thinly.</p>

<h2 id="vmware-storage-limits">VMware Storage Limits</h2>

<p>When planning your installation, keep in mind the following limits for VMFS6 as
of vSphere 6.5:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Parameter</th>
      <th style="text-align: left">Limit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Maximum iSCSI LUNs</td>
      <td style="text-align: left">512</td>
    </tr>
    <tr>
      <td style="text-align: left">Maximum software iSCSI targets</td>
      <td style="text-align: left">256</td>
    </tr>
    <tr>
      <td style="text-align: left">Maximum volume size</td>
      <td style="text-align: left">64 TB</td>
    </tr>
    <tr>
      <td style="text-align: left">Concurrent vMotion ops per volume</td>
      <td style="text-align: left">128</td>
    </tr>
    <tr>
      <td style="text-align: left">Powered on VMs per volume</td>
      <td style="text-align: left">2048</td>
    </tr>
  </tbody>
</table>

<h2 id="storage-io-control">Storage I/O Control</h2>
<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Datastore -&gt; Configure -&gt; General -&gt; Datastore Capabilities -&gt; Storage I/O Control</div>

<p>Storage I/O Control (SIOC) is VMware’s solution to the “noisy neighbor”
problem: when a single VM’s I/O load swamps the storage subsystem, negatively
affecting the performance of other VMs. SIOC allows the hypervisor to throttle
guest I/O when the latency or throughput of the storage subsystem increases
beyond a predefined point. You can specify policies for how each guest’s I/O is
scheduled when SIOC is active.</p>

<p>Blockbridge dataplanes fully support SIOC, as all of the enforcement is
performed in the ESXi host. If raw, global performance is your primary concern,
you should leave SIOC disabled. The best performance is going to happen when
VMware can issue I/O’s out to the LUN as quickly as possible. However, if you
have VMs that require predictable I/O latency, you may find that SIOC helps
meet those requirements.</p>

<p>Correctly implementing SIOC starts with understanding exactly where it sits in
the ESXi I/O stack:</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image1.jpg" alt="A Diagram of the VMware IO Stack" style="max-width: 90%" /></figure>

<blockquote>
  <p><em>Source:
<a href="https://blogs.vmware.com/vsphere/2012/07/troubleshooting-storage-performance-in-vsphere-part-5-storage-queues.html">Troubleshooting Storage Performance in vSphere Part 5: Storage Queues (VMware)</a></em></p>
</blockquote>

<p>In the above diagram, the SIOC measurement point is on top of the Driver. It
measures the round-trip latency and throughput of I/O’s issued to the device
queue. With a Blockbridge iSCSI LUN, this is the time between when ESXi hands
off the I/O to its iSCSI initiator and when it receives the response. DQLEN,
shown above, counts I/O’s that are in this state. When SIOC measures I/O
performance, it’s measuring the performance at the iSCSI LUN from this point of
view, not any latency related to queueing on the ESXi host or the world queue
above the device in the I/O stack.</p>

<p>When this latency gets too long, or when the throughput gets too high, SIOC
shortens the device queue and starts enforcing the configured I/O shares
assigned to each VM. This backs up the I/O load into the World queue, trading
off increasing latency for more fairly managed storage.  However, a major
downside of SIOC’s approach is that it reduces the effectiveness of the backing
storage by shortening the device queue.</p>

<p>Should you use SIOC, go with the default setting of triggering SIOC at a
percentage (default 90%) of maximum throughput. The minimum “manual” latency
threshold is 5ms, which is quite a long time for an all-flash array, even with
a full queue.</p>

<p>SIOC is useful when it functions as an escape valve. When properly implemented,
it shouldn’t kick in most of the time, only responding to periods of truly
intense workloads with tighter regulation of guest I/O.  But its action to
reduce the device queue depth also reduces the performance handling
capabilities of the backing storage. Tradeoffs!</p>

<div class="alert alert-success" role="alert"><i class="fa fa-check-square-o"></i> <b>Tip:</b> <strong>Our recommendation is to leave SIOC disabled</strong>
until you’ve discovered that you need it. Even if you’re experiencing the noisy
neighbor problem, it may be better to see if you can move the noisy neighbors
away to other hosts or other datastores. Or, dedicate storage to
latency-sensitive workloads.</div>

<h3 id="additional-considerations">Additional Considerations</h3>

<ul>
  <li>
    <p>Instead of SIOC, consider using per-VM IOPS limits, configurable
from VM Storage Policies. These allow for AWS-style IOPS
restrictions, with potentially more deterministic storage
performance.</p>
  </li>
  <li>
    <p>vSphere 6.5 introduced SIOC v2, configured with VM Storage Policies
instead of Disk Shares, but you can still configure SIOC v1. If you
refer to 3<sup>rd</sup> party documentation on the web, make sure you’re
looking at an appropriate version.</p>
  </li>
  <li>
    <p>SIOC won’t work on datastores that have more than one extent. If
you’ve built your datastore out of multiple LUNs, you can’t use
SIOC.</p>
  </li>
</ul>

<h3 id="resources">Resources</h3>

<ul>
  <li>
    <p><a href="https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.resmgmt.doc/GUID-37CC0E44-7BC7-479C-81DC-FFFC21C1C4E3.html">VMware vSphere: Storage I/O Control Requirements (VMware)</a></p>
  </li>
  <li>
    <p><a href="https://storagehub.vmware.com/t/vsphere-storage/vsphere-6-5-storage-1/storage-i-o-control-v2-2/">VMware: Storage I/O Control v2 (VMware)</a></p>
  </li>
  <li>
    <p><a href="https://docs.vmware.com/en/VMware-vSphere/6.7/com.vmware.vsphere.resmgmt.doc/GUID-7686FEC3-1FAC-4DA7-B698-B808C44E5E96.html">VMware vSphere: Managing Storage I/O Resources (VMware)</a></p>
  </li>
  <li>
    <p><a href="https://blogs.vmware.com/vsphere/2012/07/troubleshooting-storage-performance-in-vsphere-part-5-storage-queues.html">Troubleshooting Storage Performance in vSphere – Storage Queues (VMware)</a></p>
  </li>
  <li>
    <p><a href="http://virtualization.solutions/2017/10/01/io-queues-within-esxi/">IO queues within ESXi (Manuel Weisshaar)</a></p>
  </li>
</ul>

<h2 id="storage-distributed-resource-scheduling">Storage Distributed Resource Scheduling</h2>

<p>Storage DRS (SDRS) is a vCenter feature that offers a long-term solution to
balancing storage performance and capacity. To manage performance, SDRS samples
I/O latency over a number of hours. If sustained latency is far out of balance,
it periodically makes recommendations about virtual machines that could be
moved to balance the load. Likewise, if datastores have an imbalance of space
allocation, SDRS will recommend VMs to move. If the SDRS cluster is fully
automated, it takes initiative, migrating VMs on its own. SDRS takes actions on
roughly the granularity of a day.</p>

<p>The latency metric used by Storage DRS is measured at the same level in the I/O
stack where virtual machine I/O is injected. Referring back to the diagram in
the <a href="#storage-io-control">Storage I/O Control</a> section, it includes latency
incurred on the host-side World queue. This is a better measure of “whole
system” performance, including inefficiencies on the host.</p>

<h3 id="additional-considerations-1">Additional Considerations</h3>

<ul>
  <li>
    <p>Thin provisioning on the Blockbridge side can confuse vCenter’s
sense of how much space is available. Configure your Blockbridge
datastore with a 1:1 reservable-to-size ratio and use Blockbridge
reservation thresholds in addition to VMware datastore disk usage
alarms.</p>
  </li>
  <li>
    <p>If you’re using Blockbridge snapshots and backups, you’ll need to
coordinate them with the VM migrations. Use manual mode SDRS in this
case.</p>
  </li>
</ul>

<h3 id="resources-1">Resources</h3>

<ul>
  <li>
    <p><a href="http://www.yellow-bricks.com/2012/12/06/should-i-use-many-small-luns-or-a-couple-of-large-luns-for-storage-drs/">Should I use many small LUNs or a couple large LUNs for Storage DRS? (Duncan Epping)</a></p>
  </li>
  <li>
    <p><a href="https://kb.vmware.com/s/article/2149938">VMware Storage DRS FAQ (VMware),</a></p>
  </li>
  <li>
    <p><a href="https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vsphere-storage-drs-interoperability-white-paper.pdf">VMware Storage DRS Interoperability (VMware)</a></p>
  </li>
</ul>

<h2 id="storage-performance-classes">Storage Performance Classes</h2>

<p>Blockbridge dataplanes offer programmatically-defined performance
characteristics that give you full control over IOPS and bandwidth quality of
service. Whether your deployment consists of a single Blockbridge complex with
homogenous storage media or multiple Dataplane complexes with tiered media
types, you can configure VMware to deliver multiple classes of storage.</p>

<p>We recommend that you implement storage classes only if your environment
consists of mixed application use-cases. For example: if you would like to
prevent background processing jobs from affecting the performance of database
applications, you may opt for multiple storage classes with segregated
performance guarantees (even if they share the same pool of capacity). You
should <strong>create a separate VMware VMFS datastore for each performance class</strong>.</p>

<p>If you use Storage Distributed Resource Scheduling (SDRS), <strong>create independent
SDRS clusters for each performance class</strong>. It is essential to point out that
SDRS is not a tiering solution. You should ensure that all VMFS datastores
within an SDRS cluster have uniform performance characteristics.</p>

<h3 id="resources-2">Resources</h3>

<ul>
  <li><a href="https://frankdenneman.nl/2012/09/19/storage-drs-and-storage-profiles-part-2-distributed-vms/">VM Storage Profiles and Storage DRS – Part 2 (Frank Denneman)</a> <em>(Older but still valid information on this approach.)</em></li>
</ul>

<hr />

<h1 id="networking--storage">NETWORKING &amp; STORAGE</h1>

<h2 id="vmware-multipath-networking">VMware Multipath Networking</h2>

<p>Multipathing is a VMware datastore compliance requirement for
network-attached storage. We’ve found that it’s best to get it out of
the way before you attempt to connect to storage. Saving it for
later frequently results in phantom storage paths that need a reboot
to clear out.</p>

<p>The definitive reference for ESXi iSCSI multipath configuration is,
unfortunately, several years old: <a href="https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmware-multipathing-configuration-software-iscsi-port-binding-white-paper.pdf">Multipathing Configuration for
Software iSCSI Using Port Binding
(VMware)</a>. Still,
the document is very thorough and mostly accurate today. If you have a
complicated install, you should give it a read.</p>

<h3 id="requirements-for-common-network-architectures">Requirements for Common Network Architectures</h3>

<p>There are several common network patterns used for highly available iSCSI storage.</p>

<p><strong>Mode A: Single Subnet Nx1</strong></p>

<ul>
  <li>ESXi has N interface ports, each with an IP address on the same subnet.</li>
  <li>Blockbridge has a single logical or physical interface port configured with a single IP address.</li>
</ul>

<p><strong>Mode B: Single Subnet NxM</strong></p>

<ul>
  <li>ESXi has N interface ports, each with an IP address on the same subnet.</li>
  <li>Blockbridge has M logical or physical interface ports, each with an IP address on the same subnet.</li>
</ul>

<p><strong>Mode C: Multiple Subnet NxN</strong></p>

<ul>
  <li>ESXi has N interface ports, each with an IP address on a different subnet.</li>
  <li>Blockbridge has N logical or physical interface ports, each with an IP address on different subnets.</li>
</ul>

<p>If your ESXi interface ports are on different subnets, no additional
network configuration is required for multipathing. <strong>If your ESXi
interface ports share a subnet, you must configure iSCSI Port
Bindings.</strong></p>

<h3 id="virtual-networking---vmkernel-adapters--vswitches">Virtual Networking - VMkernel Adapters &amp; vSwitches</h3>

<p>This section offers a simplified description of the process for
configuring VMkernel Network Adapters and Virtual Switching in support
of iSCSI multipathing deployed on a single subnet. Specific attention
is paid to changes found in the vCenter 6.7 HTML interface.
Configuration via the CLI is also possible: see <a href="https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmware-multipathing-configuration-software-iscsi-port-binding-white-paper.pdf">Multipathing
Configuration for Software iSCSI Using Port Binding
(VMware)</a>
for details.</p>

<p>To start, Launch the <strong>Add Networking</strong> wizard for your host:</p>

<ol>
  <li>
    <p><strong>Select the host from the sidebar menu</strong>.</p>
  </li>
  <li>
    <p><strong>Select Configure</strong>.</p>
  </li>
  <li>
    <p><strong>Expand Networking</strong>.</p>
  </li>
  <li>
    <p><strong>Select VMkernel Adapters</strong>.</p>
  </li>
  <li>
    <p><strong>Select Add Networking</strong>.</p>
  </li>
</ol>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image2.jpg" alt="VMware screenshot showing VMkernel Network Adapters" style="max-width: 90%" /></figure>

<p><strong>Create a new VMkernel Adapter and vSwitch</strong>:</p>

<ol>
  <li>
    <p><strong>Select VMkernel Network Adapter</strong> as the connection type.</p>
  </li>
  <li><strong>Select New standard switch</strong> to create a vSwitch.
    <ul>
      <li>Set an <strong>MTU</strong> value compatible with your network.</li>
    </ul>
  </li>
  <li>
    <p><strong>Assign participating physical interfaces to the vSwitch</strong> by clicking the green “+” button. Here, we’ve added vmnic2 and vmnic1000202.</p>

    <figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image3.jpg" alt="VMware screenshot showing creation of a virtual switch" style="max-width: 90%" /></figure>
  </li>
  <li><strong>Edit the VMkernel Adapter Port properties</strong>.
    <ul>
      <li>Enable <strong>vMotion</strong> and <strong>Provisioning</strong> data services.</li>
      <li>Specify a <strong>Network Label</strong> that makes it clear that this is one of several ports used for iSCSI traffic. In our example, we chose “iSCSI pg1”.</li>
    </ul>

    <figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image4.jpg" alt="VMware screenshot showing port properties" style="max-width: 90%" /></figure>
  </li>
  <li><strong>Edit the IPv4 settings</strong>
    <ul>
      <li>enter the IP address and netmask for the VMkernel Adapter Port.</li>
    </ul>
  </li>
  <li>Finally, complete the workflow.</li>
</ol>

<p><strong>Create VMkernel Adapters for each additional physical network interface port</strong>, adding them to your existing vSwitch. This is the same process as above, <strong>substituting step 2 as follows</strong>:</p>

<ol>
  <li>
    <p>For the target device, <strong>Select an existing standard switch</strong>. Specify the vSwitch created previously.</p>

    <figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image5.jpg" alt="VMware screenshot showing add target device selection" style="max-width: 90%" /></figure>
  </li>
</ol>

<p>On completion, you should see the two VMkernel adapters:</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image6.jpg" alt="VMware screenshot showing VMkernel adapters" style="max-width: 90%" /></figure>

<h3 id="virtual-networking---physical-adapter-selection">Virtual Networking - Physical Adapter Selection</h3>

<p>At this point, you have created a <strong>vSwitch</strong> that connects multiple <strong>VMkernel Adapters</strong> with multiple <strong>Physical Adapters</strong>. By default, VMware will use a single physical adapter for outbound traffic from the vSwitch. To use additional physical adapters, you must apply a policy to each <strong>VMkernel Adapter</strong> that binds outbound traffic to single <strong>Physical Adapter</strong>.</p>

<p>To start, <strong>locate your vSwitch Network Diagram</strong></p>

<ol>
  <li><strong>Select Networking</strong> / <strong>Virtual switches</strong>.
    <ul>
      <li>Scroll down as needed to find your vSwitch.</li>
    </ul>
  </li>
</ol>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image7.jpg" alt="VMware screenshot showing newly created vSwitch" style="max-width: 90%" /></figure>

<p>Next, <strong>edit the configuration of each VMkernel adapter</strong>.</p>

<ol>
  <li>
    <p><strong>Select the three dots</strong> to the right of the first VMkernel adapter (here, it’s “iSCSI pg1”).</p>
  </li>
  <li>
    <p><strong>Select Edit Settings</strong>.</p>

    <figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image8.jpg" alt="VMware screenshot of adapter edit dialog" style="max-width: 90%" /></figure>
  </li>
  <li>
    <p><strong>Select Teaming and failover</strong> in the edit settings dialog.</p>
  </li>
  <li>
    <p><strong>Select the checkbox next to Override</strong></p>
  </li>
  <li>
    <p><strong>Assign a Single Physical Adapter as an Active Adapter</strong></p>
    <ul>
      <li>Assign all other adapters as <strong>Unused Adapters</strong>.</li>
    </ul>

    <figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image9.jpg" alt="VMware screenshot showing unused adapters" style="max-width: 90%" /></figure>
  </li>
</ol>

<p>Repeat this process for each <strong>VMkernel adapter</strong>, ensuring each adapter maps to a unique <strong>Physical Adapter</strong>. In the example below, our “iSCSI pg2” adapter assigns “vmnic1000202” as active since “iSCSI pg1” previously assigned “vmnic2”.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image10.jpg" alt="VMware screenshot showing unusued adapters" style="max-width: 90%" /></figure>

<h3 id="network-port-bindings-for-iscsi">Network Port Bindings for iSCSI</h3>

<p>At this point, you have multiple <strong>VMkernel adapters</strong> that are mapped to independent <strong>Physical Adapters</strong> configured with <strong>IP addresses on the same subnet</strong>. The last step is to configure network port bindings in the <strong>VMware Software iSCSI Adapter</strong>.</p>

<ol>
  <li>
    <p><strong>Select Storage / Storage Adapters</strong>.</p>
  </li>
  <li>
    <p><strong>Select the iSCSI Software Adapter</strong>.</p>
  </li>
  <li>
    <p><strong>Select Network Port Binding</strong>.</p>
  </li>
  <li>
    <p><strong>Select Add</strong>.</p>

    <figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image11.jpg" alt="VMware screenshot binding port groups to an iSCSI adapter" style="max-width: 90%" /></figure>
  </li>
  <li>
    <p><strong>Select your VMkernel Adapters</strong>, then <strong>select OK</strong>.</p>

    <figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image12.jpg" alt="VMware screenshot showing multiple selected adapters" style="max-width: 90%" /></figure>
  </li>
</ol>

<h3 id="resources-3">Resources</h3>

<ul>
  <li>
    <p><a href="https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/vmware-multipathing-configuration-software-iscsi-port-binding-white-paper.pdf">Multipathing Configuration for Software iSCSI Using Port Binding (VMware)</a></p>
  </li>
  <li>
    <p><a href="https://wahlnetwork.com/2015/03/09/when-to-use-multiple-subnet-iscsi-network-design/">When To Use Multiple Subnet iSCSI Network Design (Chris Wahl)</a></p>
  </li>
  <li>
    <p><a href="https://www.stephenwagner.com/2014/06/07/vmware-vsphere-iscsi-port-binding/">VMware vSphere - When to use iSCSI Port Binding, and why! (Stephen Wagner)</a></p>
  </li>
  <li>
    <p><a href="https://www.codyhosterman.com/2018/05/esxi-iscsi-multiple-subnets-and-port-binding/">ESXi iSCSI, Multiple Subnets, and Port Binding (Cody Hosterman)</a></p>
  </li>
  <li>
    <p><a href="https://kb.vmware.com/s/article/2038869">Considerations for using software iSCSI port binding in ESX/ESXi (2038869) (VMware KB)</a></p>
  </li>
  <li>
    <p><a href="https://kb.vmware.com/s/article/2010877">Multi-homing on ESXi/ESX (2010877) (VMware KB)</a></p>
  </li>
</ul>

<h2 id="lacp--bonded-networking">LACP &amp; Bonded Networking</h2>

<p>If your network configuration consists of bonded network interfaces that use
the Link Aggregation Control Protocol (LACP), we recommend that you <strong>configure
“fast rate” (also known as “lacp short-timeout”)</strong>. “Fast rate” improves link
failure recovery times from the default value of 30 seconds to 1 second. Proper
setup requires consistent configuration settings on your host interfaces and
corresponding switch ports.</p>

<p>We also recommend that you <strong>configure passive transmit hashing on bonded
interfaces</strong>. Transmit hashing ensures that the ethernet packets of a single
network flow transmit on the same network link. Transmit hashing reduces the
probability of packet reordering and performance loss for iSCSI/TCP. We
recommend that you incorporate both L3 (i.e., source and destination IPv4 and
IPv6 addresses) and L4 (i.e., source and destination ports) in your hash
configuration for efficient flow distribution across your network links.</p>

<p>The example below shows a Linux team configuration with active/active LACP and
l3/l4 transmit hashing.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># teamdctl t0 config dump
{
    "device": "t0",
    "link_watch": {
        "name": "ethtool"
    },
    "ports": {
        "fe0": {},
        "fe1": {}
    },
    "runner": {
        "active": true,
        "fast_rate": true,
        "name": "lacp",
        "tx_hash": [
            "l3",
            "l4"
        ]
    }
}
</code></pre></div></div>

<h2 id="provisioning-iscsi-storage">Provisioning iSCSI Storage</h2>

<p>This section describes how to provision and configure your Blockbride storage
for use with a VMware datastore. You’ll create one or more virtual storage
services distributed across your dataplane complexes. Inside those storage
services, you will create a virtual disk and an iSCSI target for each VMware
datastore that you want to create. A global set of CHAP credentials will serve
to authenticate your VMware cluster to the Blockbridge dataplane endpoints.</p>

<p>Start by logging in to the administrative “system” user in the Blockbridge web
GUI. On the infrastructure tab, manually provision a virtual storage service
(VSS) on each dataplane complex that will connect to your VMware
installation. If you plan to create multiple VMware datastores per dataplane
complex, create the disks to back them inside a single virtual service.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image14.jpg" alt="Blockbridge screenshot showing flyout menu of a datastore" style="max-width: 90%" /></figure>

<p>Select “Manually provision virtual service”. On the dialog window that pops up,
create a label for the service that references the label of the dataplane
complex. You may create the service inside the “system” account, or you may
wish to create a dedicated account for your VMware storage.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image15.jpg" alt="Blockbridge screenshot showing storage service provisioning modal" style="max-width: 90%" /></figure>

<p>Reserve enough size to hold the contents of the VMware datastores you intend to
create. If you created the VSS on a different account (as we did with the “vmw”
account here), log out, and log back in as that account.</p>

<p>The easiest way of managing authentication for your VMware deployment is to use
a global initiator profile. Create one from the flyout menu off of Global, in
the Storage tab.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image16.jpg" alt="Blockbridge screenshot showing flyout menu of a storage service" style="max-width: 90%" /></figure>

<p>In the “Create Initiator Profile” dialog that opens, enter a CHAP username
(here, “esx@vmw”) and a CHAP secret, with confirmation.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image17.jpg" alt="Blockbridge screenshot showing create initiator profile modal" style="max-width: 90%" /></figure>

<p>You will need the iSCSI IQN from the iSCSI adapter in each ESXi host.</p>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; Storage/Storage Adapters -&gt; IQN</div>

<p>Paste each host’s IQN into the “Permitted Initiators” section of the dialog
then click “create”.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image18.jpg" alt="VMware screenshot showing where to find the iSCSI initiator IQN" style="max-width: 90%" /></figure>

<p>Next, create one or more disks in the service to host your VMFS datastores. If
you have several disks to create, you may wish to move to the Blockbridge CLI
tool to do it. Both the GUI and CLI are covered below.</p>

<p>Select “Create a disk” from the storage service’s flyout menu, shown below.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image19.jpg" alt="Blockbridge screenshot showing the flyout menu of a storage service" style="max-width: 90%" /></figure>

<p>Enter the size of the disk in the dialog that pops up, along with an
appropriate label. Click “create”.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image20.jpg" alt="Blockbridge screenshot showing the create disk modal" style="max-width: 90%" /></figure>

<p>From the CLI, it’s:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bb disk create --vss cx1:nvme --label ds1 --capacity 1TiB
</code></pre></div></div>

<p>Next, create an iSCSI target on the Blockbridge side for each disk you created
above.  Creating one target per disk ensures that each LUN has an independent
iSCSI command queue.  Select “Create a target” from the virtual storage
service’s flyout menu (shown earlier).</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image21.jpg" alt="Blockbridge screenshot showing the create disk modal" style="max-width: 90%" /></figure>

<p>On each target, select “insert” to add one disk. Select the label of the global
initiator profile you created earlier to grant access via those credentials.</p>

<p>Creating a target is a multi-step process from the CLI:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bb target create --vss cx1:nvme --label target
bb target acl add --target target --profile "cluster profile"
bb target lun map --target target --disk ds1
</code></pre></div></div>

<p>Repeat this procedure for each disk.</p>

<h2 id="vmware-initiator-configuration">VMware Initiator Configuration</h2>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; Storage/Storage Adapters</div>

<p>From the vSphere GUI, select the iSCSI adapter. Under Properties, scroll down
to Authentication. Select <strong>Edit…</strong></p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image22.jpg" alt="VMware screenshot showing where to edit iSCSI authentication settings" style="max-width: 90%" /></figure>

<p>Enter the CHAP username and secret from the Blockbridge initiator
profile. Blockbridge targets support mutual authentication – a good idea if
you’re concerned about an attacker interposing a illegitimate iSCSI target in
your infrastructure.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image23.jpg" alt="VMware screenshot showing the edit iSCSI authentication modal" style="max-width: 50%" /></figure>

<p>Note that VMware’s iSCSI software adapter is limited to one set of “parent”
CHAP authentication credentials that can be inherited into the static and
dynamic targets. If you need to use other iSCSI targets with different
credentials, you can unclick “Inherit authentication settings from parent” and
enter a new set of CHAP credentials each time you add a target.</p>

<h3 id="dynamic-discovery">Dynamic Discovery</h3>

<p>Staying on the iSCSI software adapter, select Dynamic Discovery, then click
<strong>Add…</strong></p>

<div class="alert alert-success" role="alert"><i class="fa fa-check-square-o"></i> <b>Tip:</b> In most cases, you can (and should) use Dynamic
    Discovery. However, VMware’s dynamic discovery doesn’t support target ports
    other than 3260.</div>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/dynamic-discovery.jpg" alt="VMware screenshot showing where to manage iSCSI dynamic discovery" style="max-width: 90%" /></figure>

<p>On the dialog that pops up, enter one of the Blockbridge target’s portal IP
addresses, along with the port.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/sendtarget-server.jpg" alt="VMware screenshot showing dynamic iSCSI target server modal" style="max-width: 50%" /></figure>

<p>Alternatively, add the dynamic discovery target with esxcli:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli iscsi adapter discovery sendtarget add --adapter=vmhba64 --address=172.16.200.44:3260
</code></pre></div></div>

<h3 id="static-discovery">Static Discovery</h3>

<p>In the rare instance that you need to use static target discovery, here’s how
to do it.  Select Static Discovery, then click <strong>Add…</strong></p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image24.jpg" alt="VMware screenshot showing where to manage iSCSI static discovery" style="max-width: 90%" /></figure>

<p>On the dialog that pops up, enter one of the Blockbridge target’s portal IP
addresses, along with the port, and the target’s IQN. Note that the port in
this case is 3261.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image25.jpg" alt="VMware screenshot showing static iSCSI target server modal" style="max-width: 50%" /></figure>

<p>With static discovery, you have to add each path manually. You may find it
easier to use esxcli to add static targets. The following example adds paths
for two target portal IP addresses.</p>

<div class="alert alert-success" role="alert"><i class="fa fa-check-square-o"></i> <b>Tip:</b> VMware’s older documentation is quicker to
    navigate than the new stuff. Here’s a document with numerous helpful iSCSI
    CLI configuration examples:
    <a href="https://pubs.vmware.com/vsphere-50/index.jsp?topic=%2Fcom.vmware.vcli.examples.doc_50%2Fcli_manage_iscsi_storage.7.5.html">vSphere 5 Command Line Documentation (VMware)</a></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli iscsi adapter discovery statictarget add --adapter=vmhba64 --address=172.16.200.44:3261 \
       --name=iqn.2009-12.com.blockbridge:t-pjwahzvbab-nkaejpda:target

esxcli iscsi adapter discovery statictarget add --adapter=vmhba64 --address=172.16.201.44:3261 \
       --name=iqn.2009-12.com.blockbridge:t-pjwahzvbab-nkaejpda:target
</code></pre></div></div>

<h3 id="per-target-chap">Per-Target CHAP</h3>

<p>If you need per-target CHAP settings, update the target with the following
command. It will prompt you to enter the secret, so it doesn’t show up in
command line logs.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli iscsi adapter target portal auth chap set --adapter=vmhba64 \
       --direction=uni --authname=esx@vmw \
       --secret --level=required --address=172.16.200.44:3261 \
       --name=iqn.2009-12.com.blockbridge:t-pjwahzvbab-nkaejpda:target
</code></pre></div></div>

<h3 id="path-selection-policy">Path Selection Policy</h3>

<p>Next up, change the multipathing policies for each LUN to <strong>Round Robin</strong>. From
the vSphere GUI, click <strong>Edit Multipathing…</strong> then change the policy.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image26.jpg" alt="VMware screenshot showing where to find multipath settings" style="max-width: 90%" /></figure>

<p>Alternatively, from esxcli, list the devices with:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp device list
</code></pre></div></div>

<p>And set the roundrobin policy on a device with:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp device set --psp=VMW_PSP_RR --device=naa.60a010a071105fae1962194c40626ca8
</code></pre></div></div>

<p>Note that the <code class="highlighter-rouge">--psp</code> and <code class="highlighter-rouge">--device</code> options must be specified in the order
shown. The command inexplicably fails if you swap them.</p>

<p>To change the path selection policy of all “naa.” devices in bulk, wrap the
command in a for loop, as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for dev in $(esxcli storage nmp device list | egrep ^naa); do
    esxcli storage nmp device set --psp=VMW_PSP_RR --device=${dev}
done
</code></pre></div></div>

<div class="alert alert-success" role="alert"><i class="fa fa-check-square-o"></i> <b>Tip:</b> For more regarding path selection configuration,
refer to the <a href="#host-tuning">Host Tuning</a> section below.</div>

<p>Finally, verify that all paths to each device are working. Select <strong>Storage
Devices -&gt; Paths</strong>. Each path should show Active status.  Neither path should
have any indicator in the “Preferred” column.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image27.jpg" alt="VMware screenshot showing where view storage path health" style="max-width: 90%" /></figure>

<h2 id="creating-a-vmfs-datastore">Creating a VMFS Datastore</h2>

<p>Create one VMFS datastore for each disk created above.</p>

<p>Whenever possible, use VMFS version 6. VMFS-6, introduced with vSphere 6.5,
includes numerous performance and stability improvements. In particular, the
following enhancements are notable for Blockbridge installations:</p>

<ul>
  <li>
    <p>VMFS-6 metadata is aligned to a 4 KiB blocksize.</p>
  </li>
  <li>
    <p>It has improved miscompare handling for ATS heartbeats,</p>
  </li>
  <li>
    <p>reduced lock contention, and</p>
  </li>
  <li>
    <p>supports multiple concurrent transactions.</p>
  </li>
</ul>

<p>We recommend VMFS-6 for all new installations.</p>

<p>To create the datastore, right-click on a host then select <strong>Storage -&gt; New
Datastore</strong>. We recommend creating the datastore from a single disk and setting
its size to consume 100% of that disk.</p>

<h3 id="resources-4">Resources</h3>

<ul>
  <li><a href="https://storagehub.vmware.com/t/vsphere-storage/vsphere-6-5-storage-1/vmfs-6-5/">VMware VMFS-6 Notes (VMwware)</a></li>
</ul>

<hr />

<h1 id="host-tuning">HOST TUNING</h1>

<h2 id="iscsi-lun-queue-depth">iSCSI LUN Queue Depth</h2>

<p>The iSCSI <strong>LunQDepth</strong> parameter controls the number of concurrent I/O
operations that ESXi can issue on a single iSCSI session before queuing occurs
in the host. We recommend that you increase LunQDepth to 192 for a Blockbridge
LUN. The default value of LunQDepth is 128.</p>

<p>The risk of setting this number too low is obvious: performance suffers because
vSphere can’t get enough work out into the LUN. There really isn’t a practical
downside to increasing the value to 192. It’s always better to get the I/O out
to the storage subsystem, rather than leave it queued on the host.</p>

<p>Increasing the queue depth requires a host reboot. Use the following esxcli
command to increase the depth, then reboot the ESXi host.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli system module parameters set -m iscsi_vmk -p iscsivmk_LunQDepth=192
</code></pre></div></div>

<p>After reboot, validate that your iSCSI devices have the increased “Device Max
Queue Depth”.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage core device list
...
naa.60a010a03ff1bb511962194c40626cd1
   Display Name: B\*BRIDGE iSCSI Disk (naa.60a010a03ff1bb511962194c40626cd1)
   Has Settable Display Name: true
   Size: 1048576
   ...
   Device Max Queue Depth: 192
   No of outstanding IOs with competing worlds: 32
</code></pre></div></div>

<p>In a Linux guest, spin up this 192 queue depth, 4K random read workload with
fio. This example uses reads on the root disk. Do not change this test to
write.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[global]
rw=randreadT
bs=4096
iodepth=192
direct=1
ioengine=libaio
time_based
runtime=180
numjobs=1

[local]
filename=/dev/sda
</code></pre></div></div>

<p>From the ESXi console, run esxtop and press “u” to get the storage view.
Validate that more than 128 commands are outstanding in the ACTV column:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>9:42:43pm up 13 min, 551 worlds, 2 VMs, 6 vCPUs; CPU load average: 1.46, 1.49, 0.00

DEVICE                                PATH/WORLD/PARTITION DQLEN WQLEN ACTV QUED %USD  LOAD    CMDS/s   READS/s  WRITES/s
naa.60a010a03ff1bb511962194c40626cd1           -             192     -  210    0   82  0.82 232971.42 232971.23      0.00
</code></pre></div></div>

<h2 id="schednumreqoutstanding-depth">SchedNumReqOutstanding Depth</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage core device set --sched-num-req-outstanding
</code></pre></div></div>

<p>vSphere has a special setting that controls how deep the I/O queue is for a
guest when other guests are accessing the same storage device. In earlier
versions of ESXi, this used to be controlled via the global parameter
<strong>Disk.SchedNumReqOutstanding</strong>. But starting in 5.5, control has been
relegated to an esxcli-only parameter, viewable in the output of esxcli storage
core device list: “No of outstanding IOs with competing worlds”, like this;</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@esx:~] esxcli storage core device list
naa.60a010a0b139fa8b1962194c406263ad
   Display Name: B*BRIDGE iSCSI Disk (naa.60a010a0b139fa8b1962194c406263ad)
   ...
   Device Max Queue Depth: 128
   No of outstanding IOs with competing worlds: 32
</code></pre></div></div>

<p>If only one guest is sending I/O to a storage device, it’s permitted to use the
full queue depth (128, or 192 if increased as recommended.) As soon as a second
guest begins accessing the device, by default, the queue depth of each guest
drops to 32.</p>

<p>Generally, we recommend increasing this setting to 64 if you’ve increased the
iSCSI LUN queue depth to 192. In situations where multiple guests are accessing
a device at the same time, the queue depth of 64 ensures that no guest claims
more than their fair share of the storage performance, yet still has enough
depth to get commands out to the LUN.  And, as will be the case much of the
time, when only one guest is accessing the device, it can do so with the full
depth of the device’s queue.</p>

<p>In some situations, even this behavior is not desirable. You can confirm this
is happening to your guests by running esxtop and watching the DQLEN column. If
it’s stuck at 32 (or 64) for multiple guests, it’s a safe bet they’re subject
to this parameter. If you believe that your guest workloads are unfairly
penalized by this setting, try increasing it to the device queue depth (192).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@esx:~] esxcli storage core device set -sched-num-req-outstanding 192 -d naa.60a010a0b139fa8b1962194c406263ad
</code></pre></div></div>

<h2 id="queue-depth-full">Queue Depth Full</h2>
<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; System / Advanced System Settings: Disk.QFullSampleSize</div>

<p>vSphere can dynamically reduce its queue size if the backing storage reports a
SCSI TASK SET FULL or BUSY status. This behavior allows it to adapt to arrays
with shallow queue depths. But with the default queue depth of 128 or even 192,
the Blockbridge dataplane’s queue isn’t going to report this status. Our
recommendation is to <strong>leave the Disk.QFullSampleSize parameter set to zero</strong>
(its default), disabling this feature.</p>

<h2 id="round-robin-path-selection-iops-limit">Round Robin Path Selection IOPS Limit</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp psp roundrobin deviceconfig set ...
</code></pre></div></div>

<p>By default, the round-robin iSCSI multipathing plug-in sends 1,000 I/O’s down
one active path before switching to another. This technique often fails to
unlock the full bandwidth of multiple paths. With a queue depth of 128 or 192,
the workload “sloshes” back and forth between the two paths, rather than
saturating both of them. By lowering the I/O limit to 8, VMware switches paths
after every eight I/O’s issued, more efficiently using the network.</p>

<p>Quite a few vendors recommend lowering this limit to 1 I/O. However, there are
some processing efficiencies to be had by staying on the same path for several
I/O’s in a row. Notably, using a slightly larger setting (like 8) interacts
favorably with NIC receive coalescing on the Blockbridge side for writes, and
on the ESXi side for reads.</p>

<p>The change can only be made from the command line. Use esxcli storage nmp
device list to display the SCSI ID’s of your devices and their current path
selection policies:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp device list

naa.60a010a03ff1bb511962194c40626cd1
   Device Display Name: B*BRIDGE iSCSI Disk (naa.60a010a03ff1bb511962194c40626cd1)
   Storage Array Type: VMW_SATP_DEFAULT_AA
   Storage Array Type Device Config: {action_OnRetryErrors=off}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=rr,iops=1000,bytes=10485760,useANO=0; lastPathIndex=0: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba64:C1:T0:L0, vmhba64:C0:T0:L0
   Is USB: false
</code></pre></div></div>

<p>Set the type to “iops” and the limit to 8:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp psp roundrobin deviceconfig set --type=iops --iops=8 \
       --device=naa.60a010a03ff1bb511962194c40626cd1
</code></pre></div></div>

<p>View the results:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp device list

naa.60a010a03ff1bb511962194c40626cd1
   Device Display Name: B*BRIDGE iSCSI Disk (naa.60a010a03ff1bb511962194c40626cd1)
   Storage Array Type: VMW_SATP_DEFAULT_AA
   Storage Array Type Device Config: {action_OnRetryErrors=off}
   Path Selection Policy: VMW_PSP_RR
   Path Selection Policy Device Config: {policy=iops,iops=8,bytes=10485760,useANO=0; lastPathIndex=1: NumIOsPending=0,numBytesPending=0}
   Path Selection Policy Device Custom Config:
   Working Paths: vmhba64:C1:T0:L0, vmhba64:C0:T0:L0
   Is USB: false
</code></pre></div></div>

<p>Alternatively, view the path selection policy directly,</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli storage nmp psp roundrobin deviceconfig get -d naa.60a010a03ff1bb511962194c40626cd1

   Byte Limit: 10485760
   Device: naa.60a010a03ff1bb511962194c40626cd1
   IOOperation Limit: 8
   Latency Evaluation Interval: 0 milliseconds
   Limit Type: Iops
   Number Of Sampling IOs Per Path: 0
   Use Active Unoptimized Paths: false
</code></pre></div></div>

<h3 id="resources-5">Resources</h3>

<ul>
  <li><a href="https://kb.vmware.com/s/article/2069356">Adjusting Round Robin IOPS limit from default 1000 to 1 (2069356) (VMware)</a></li>
</ul>

<h2 id="vaai-commands">VAAI Commands</h2>

<p>The Blockbridge dataplane fully supports the VAAI command set, including:</p>

<ul>
  <li>
    <p>offloaded storage vMotion with EXTENDED COPY,</p>
  </li>
  <li>
    <p>server-side locking with COMPARE AND WRITE, and</p>
  </li>
  <li>
    <p>server-side zeroing with WRITE SAME 10 and 16.</p>
  </li>
</ul>

<p>All of these parameters should be enabled with a value of “1”, under Advanced
System Settings. By default, they’re enabled, so no change should be required.</p>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; System / Advanced System Settings: DataMover.HardwareAcceleratedInit</div>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; System / Advanced System Settings: DataMover.HardwareAcceleratedMove</div>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; System / Advanced System Settings: VMFS3.HardwareAcceleratedLocking</div>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image28.jpg" alt="VMware screenshot showing advanced system settings for hardware acceleration" style="max-width: 90%" /></figure>

<h2 id="ats-heartbeating-vmfs-6">ATS Heartbeating (VMFS-6)</h2>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; System / Advanced System Settings: VMFS3.UseATSForHBOnVMFS5</div>

<p>Keep this parameter set to the default value of 1 for ESXi 6.5 or newer
installations with VMFS-6 volumes. Blockbridge fully supports the SCSI “atomic
test and set” COMPARE AND WRITE command. (It’s also used for VAAI Storage
vMotion.) It doesn’t cause any notable load on the data plane. The legacy
alternative to ATS heartbeating is more cumbersome.</p>

<p>Versions of ESXi earlier than 6.5, or those using VMFS-5 volumes, may not have
properly handled ATS timeouts, incorrectly registering “miscompares”.  For
these older versions of ESXi, it’s best to disable this setting.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image29.jpg" alt="VMware screenshot showing advanced system settings for ATS heartbeating" style="max-width: 90%" /></figure>

<h3 id="resources-6">Resources</h3>

<ul>
  <li><a href="https://cormachogan.com/2017/08/24/ats-miscompare-revisited-vsphere-6-5/">ATS Miscompare Revisited in vSphere 6.5 (Cormac Hogan)</a>)</li>
</ul>

<h2 id="halt-vms-on-out-of-space-conditions">Halt VMs on Out-of-Space Conditions</h2>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; System / Advanced System Settings: Disk.ReturnCCForNoSpace</div>

<p>If a volume backing a datastore runs out of space, VMware thoughtfully pauses
any VM that attempts to allocate storage, instead of passing the error
through. Many applications do not handle out-of-space particularly well, so
this option can be a lifesaver. Documents from VMware refer to this as the
“thin provisioning stun” feature. It’s <strong>enabled</strong> by default, with
<strong>Disk.ReturnCCForNoSpace = 0</strong>.  Setting ReturnCCForNoSpace to “0” instructs
VMware to <em>not</em> return an error (a SCSI CHECK CONDITION) when it runs out of
space. We recommend that you leave this set to 0, allowing it to pause the VMs.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image30.jpg" alt="VMware screenshot showing advanced system settings for ReturnCCForNoSpace" style="max-width: 90%" /></figure>

<p>The SCSI status 0x7/0x27/0x7 is SPACE ALLOCATION FAILED – WRITE PROTECT.</p>

<h2 id="jumbo-frames">Jumbo Frames</h2>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; Networking / VMkernel adapters</div>

<p>You <strong>may</strong> be able to squeeze out the last 5% of performance by switching to
jumbos. But they can come with significant costs, both in implementation and in
the debug time associated with strange problems.  If you’re already using jumbo
frames at L2, you’ll need to make sure you’ve configured vSphere for jumbo
frames. If you’re not using jumbos yet, consider carefully whether you want to
undertake the transition.</p>

<ol>
  <li>
    <p>Navigate to <strong>Host -&gt; Configure -&gt; Networking / VMkernel adapters</strong></p>
  </li>
  <li>
    <p>For each adapter the must be modified:</p>

    <p>a.  Click the pencil icon,</p>

    <p>b.  Select “NIC settings”,</p>

    <p>c.  Change the MTU to 9000, and</p>

    <p>d.  Press “OK”.</p>
  </li>
  <li>
    <p>Navigate to “Virtual switches”</p>
  </li>
  <li>
    <p>For each vSwitch to be modified:</p>

    <p>a.  Click the pencil icon,</p>

    <p>b.  Under “Properties”, change the MTU to 9000, and</p>

    <p>c.  Press “OK”.</p>
  </li>
</ol>

<p>Use vmkping to test that jumbo frames are configured.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vmkping -ds 8972 &lt;IP of Blockbridge storage service&gt;
</code></pre></div></div>

<h3 id="resources-7">Resources</h3>

<ul>
  <li><a href="https://kb.vmware.com/s/article/1003728">Testing VMkernel network connectivity with the vmkping command (1003728) (VMware)</a></li>
</ul>

<h2 id="iscsi-login-timeout">iSCSI Login Timeout</h2>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; Storage / Storage Adapters -&gt; iSCSI Software
Adapter -&gt; Advanced Options: LoginTimeout</div>

<p>Plan for a Blockbridge dataplane failover time of 30 seconds. This is
transparent for applications and virtual machines, so long as the timeouts are
set to be long enough. During a dataplane failover, ESXi’s iSCSI adapter
notices that the session is unresponsive, typically within 10 seconds. It
attempts to reconnect and login. The LoginTimeout setting must be long enough
to successfully ride out a failover. We recommend <strong>60 seconds</strong>, to be on the
safe side.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image31.jpg" alt="VMware screenshot show iSCSI storage adapter setting for LoginTimeout" style="max-width: 90%" /></figure>

<p>From the CLI:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli iscsi adapter param set --adapter=vmhba64 --key=LoginTimeout --value=60
</code></pre></div></div>

<h2 id="tcp-delayedack">TCP DelayedAck</h2>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>Host -&gt; Configure -&gt; Storage / Storage Adapters -&gt; iSCSI Software
Adapter -&gt; Advanced Options: DelayedAck</div>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>iHost -&gt; Configure -&gt; Storage / Storage Adapters -&gt; iSCSI Software
Adapter -&gt; Targets -&gt; (Static/Dynamic) Discovery -&gt; Advanced: DelayedAck</div>

<p>DelayedAck was intended to boost performance by reducing the number of TCP
segment acknowledgements. Several years back, this triggered some strange
performance problems with certain iSCSI arrays with very custom TCP/IP
stacks. These arrays were effectively acknowledging every other TCP segment,
and could be left waiting periodically for VMware’s delayed ACK.</p>

<p>Blockbridge isn’t subject to any problems related to the use (or non-use) of
DelayedAck. Our stance is that it’s very unlikely to make a difference either
way.</p>

<p>If you choose, you can disable DelayedAck for the iSCSI adapter or for
individual targets. The per-adapter setting is on the <strong>Advanced Options</strong> tab
for the iSCSI software adapter. Drill down a couple more layers to access the
per-target setting, by clicking on the <strong>Targets</strong> tab in the adapter, then
<strong>Dynamic Discovery</strong> or <strong>Static Discovery</strong>, select the appropriate target,
and click <strong>Advanced</strong>. Un-check <strong>Inherit</strong> for the setting, then disable it.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image32.jpg" alt="VMware screenshot show iSCSI advanced settings for DelayedAck" style="max-width: 90%" /></figure>

<h2 id="large-receive-offload-maximum-length">Large Receive Offload Maximum Length</h2>

<p>The <strong>/Net/VmxnetLROMaxLength</strong> parameter sets the size of the Large Receive
Offload (LRO) buffer. By default, it’s set to 32,000 bytes. Increasing the size
of this <strong>may</strong> improve throughput.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcfg-advcfg -s 65535 /Net/VmxnetLROMaxLength
</code></pre></div></div>

<h2 id="nic-interrupt-balancing">NIC Interrupt Balancing</h2>

<p>Leave NIC interrupt balancing set to the default: <strong>enabled</strong>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli system settings kernel set -s intrBalancingEnabled -v true
</code></pre></div></div>

<p>Yes, to achieve the lowest possible latency, you ideally want explicit control
over the CPU core where interrupts are processed, and also where everything
else on the system is scheduled. But, it’s very difficult to capture and
maintain that kind of control on an ESXi server. We recommend that you accept
the default balancing. ESXi appears to do a decent job of avoiding cores under
heavy utilization by VMs.</p>

<h2 id="mellanox-specific-optimizations">Mellanox Specific Optimizations</h2>

<p>On ConnectX-3 NICs, we recommend disabling adaptive receive interrupt
moderation. On newer cards, it behaves well. But it doesn’t seem to quite do
the right thing on these older NICs. In addition to this change, increase the
size of the NIC’s ring buffer, explicitly eliminate transmit coalescing, and
set a modest value of 3 microseconds for receive coalescing. Be sure to do it
for all ports.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli network nic ring current set -r 4096 -n vmnicX
esxcli network nic coalesce set –-tx-usecs=0 –-rx-usecs=3 –-adaptive-rx=false -n vmnicX
</code></pre></div></div>

<p>Verify the tunings:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># esxcli network nic coalesce  get
NIC           RX microseconds  RX maximum frames  TX microseconds  TX Maximum frames  Adaptive RX  Adaptive TX  Sample interval seconds
------------  ---------------  -----------------  ---------------  -----------------  -----------  -----------  -----------------------
vmnic0        N/A              N/A                N/A              N/A                N/A          N/A          N/A
vmnic1        N/A              N/A                N/A              N/A                N/A          N/A          N/A
vmnic1000202  3                15                 0                1                  Off          Off          0
vmnic2        3                15                 0                1                  Off          Off          0
</code></pre></div></div>

<p>This optimization reduced the latency observed in the guest of a queue depth 1
4K read workload from <strong>90us</strong> to <strong>70us</strong>!</p>

<p>On ConnectX-4 or newer NICs, increase the size of the ring buffer, enable
adaptive-rx, and disable transmit coalescing:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>esxcli network nic ring current set -r 4096 -n vmnicX
esxcli network nic coalesce set ---tx-usecs=0 ---adaptive-rx=true -n vmnicX
</code></pre></div></div>

<hr />

<h1 id="guest-tuning">GUEST TUNING</h1>

<h2 id="paravirtual-scsi-adapter">Paravirtual SCSI Adapter</h2>

<p>Whenever possible, select the <strong>Paravirtual SCSI adapter</strong> for the <strong>SCSI
controller</strong> in each guest VM. This adapter offers the best performance and
lowest CPU utilization of any of the available options.  It’s also the only
virtual adapter with a queue depth larger than 32 per device.</p>

<figure style="text-align:center;"><img class="docimage" align="middle" src="./images/image33.jpg" alt="VMware screenshot showing guest virtual hardware settings" style="max-width: 50%" /></figure>

<p>Linux kernels going all the way back to 2.6.33 include the necessary
vmw_pvscsi driver. For Microsoft Windows guests, install VMware tools.</p>

<p>Consult VMware’s KB article (below) for details on changing the queue depths of
the Paravirtual SCSI adapter inside the guest OS. Unlike the defaults in the
article, recent Linux installs seem to be defaulting to 190 queue depth with 32
ring pages, so your installation may not need additional tuning:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[root@esx]# cat /sys/module/vmw_pvscsi/parameters/cmd_per_lun
190
[root@esx]# cat /sys/module/vmw_pvscsi/parameters/ring_pages
32
</code></pre></div></div>

<h3 id="resources-8">Resources</h3>

<ul>
  <li><a href="https://kb.vmware.com/s/article/2053145&gt;">Large-scale workloads with intensive I/O patterns might require queue depths significantly greater than Paravirtual SCSI default values (2053145) (VMware)</a></li>
</ul>

<h2 id="virtual-machine-encryption">Virtual Machine Encryption</h2>

<p>We recommend avoiding encrypted VMware virtual disks, for a few reasons:</p>

<ul>
  <li>
    <p>Data stored in Blockbridge is always encrypted at rest.</p>
  </li>
  <li>
    <p>Blockbridge data encryption is done in the storage hardware; your
hypervisors won’t spend any CPU cycles on the cryptography.</p>
  </li>
  <li>
    <p>Blockbridge dataplanes will not be able to use data reduction
techniques on encrypted data.</p>
  </li>
</ul>

<p>If your application requires a secure iSCSI transport between the ESXi
host and the Blockbridge dataplane, please contact Blockbridge support.</p>

<p>(Encryption is a relatively new feature, introduced in vSphere 6.5.)</p>

<h2 id="zeroing-policies">Zeroing Policies</h2>

<p>VMware famously has three types of guest disks: <strong>Thin, Lazy Zeroed</strong>, and
<strong>Eager Zeroed</strong>. With a Blockbridge array, <strong>Thin</strong> disks are nearly always
the right choice.</p>

<p>There are three factors at play in our recommendation:</p>

<ol>
  <li>
    <p>Blockbridge doesn’t allocate storage for zeroes. VMware uses VAAI zeroing
commands (SCSI WRITE SAME) for the two <strong>Zeroed</strong> disk types. If the
regions of disk it’s zeroing are not allocated, Blockbridge doesn’t bother
allocating the storage. It’s already zeroed.</p>
  </li>
  <li>
    <p>VMware has its own VMFS metadata to allocate. Though these operations are
fast, it’s a consideration. VMFS has to track the block allocations, so
doing <strong>Eager Zeroed</strong> would get these all out of the way up front.</p>
  </li>
  <li>
    <p><strong>Lazy Zeroed</strong> disks zero the storage, then write it. The write is
serialized behind the zeroing operation. Instead of simply sending the
write along to the Blockbridge LUN, where it can allocate storage
optimally, it takes an additional round trip to allocate storage first
before it can be written. Sure, the zeroing is very fast, but it’s still
additional latency. This is typically slower than the Thin disk
performance, where you “just write it”.</p>
  </li>
</ol>

<p>Blocks on <strong>Thin</strong> disks are always allocated on demand. There’s no zeroing
done ahead of time, or even just-in-time. In most cases, this is
preferable. They don’t take up space that they don’t need, and they don’t have
the write serialization penalty of <strong>Lazy Zeroed</strong> disks.</p>

<h2 id="guest-io-latency--consistency">Guest I/O Latency &amp; Consistency</h2>

<p>Achieving the lowest possible latency inside a guest requires dedicating CPU
resources to it. For example, to dedicate a CPU core to a guest:</p>

<ol>
  <li>
    <p>Select “<strong>High</strong>” <strong>Latency Sensitivity:</strong></p>
  </li>
  <li>
    <p>Add a CPU frequency <strong>Reservation</strong> for the full frequency of the CPU:</p>
  </li>
  <li>
    <p>Set the <strong>Scheduling</strong> <strong>Affinity</strong> to a CPU core that has not been
dedicated to another VM:</p>
  </li>
</ol>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>VM -&gt; Edit Settings -&gt; VM Options -&gt; Advanced</div>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>VM -&gt; Edit Settings -&gt; Virtual Hardware -&gt; CPU -&gt; Reservation</div>

<div class="alert alert-success" role="alert"><i class="fa fa-window-maximize"></i><b> VMware : </b>VM -&gt; Edit Settings -&gt; Virtual Hardware -&gt; CPU -&gt; Scheduling Affinity</div>



    <div class="tags">
        
    </div>

</div>

<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2020 Blockbridge Networks LLC. All rights reserved. <br />
 Site last generated: Feb 11, 2020
                </div>
            </div>
</footer>


          </main>

          <!-- /.col-body -->
        </div>

        <!-- /.row -->
      </div>
      <!-- /#main -->
    </div>
  </div>

</body>

</html>
